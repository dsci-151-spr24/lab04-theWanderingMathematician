---
title: "Lab 04 - La Quinta is Spanish for next to Denny's, Pt. 1"
subtitle: "Visualizing spatial data"
author: "Micaiah Balonek"
output: 
  tufte::tufte_html:
    tufte_variant: "envisioned"
    highlight: pygments
    css: ../lab.css
link-citations: true
---

```{r include = FALSE}
knitr::opts_chunk$set(eval = TRUE)
```

```{r fig.margin = TRUE, echo = FALSE}
knitr::include_graphics("img/mitch-hedgeberg-lqd.jpg")
```

Have you ever taken a road trip in the US and thought to yourself "I wonder what La Quinta means".
Well, the late comedian [Mitch Hedberg](https://en.wikipedia.org/wiki/Mitch_Hedberg) thinks it's Spanish for *next to Denny's*.

If you're not familiar with these two establishments, [Denny's](https://www.dennys.com/) is a casual diner chain that is open 24 hours and [La Quinta Inn and Suites](http://www.lq.com/) is a hotel chain.

These two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg.
In this lab we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data.

The inspiration for this lab comes from a blog post by John Reiser on his *new jersey geographer* blog.
You can read that analysis [here](http://njgeo.org/2014/01/30/mitch-hedberg-and-gis/).
Reiser's blog post focuses on scraping data from Denny's and La Quinta Inn and Suites websites using Python.
In this lab we focus on visualization and analysis of these data.
However note that the data scraping was also done in R, and we we will discuss web scraping using R later in the course.
But for now we focus on the data that has already been scraped and tidied for you.

# Learning goals

-   Visualising spatial data
-   Joining data frames

# Getting started

[Go to the github repo and bring it into Posit Cloud.](https://classroom.github.com/a/82bhDG2j) 

## Warm up

Before we introduce the data, let's warm up with some simple exercises.

-   Update the YAML, changing the author name to your name, and **knit** the document.
-   Commit your changes with a meaningful commit message.
-   Push your changes to GitHub.
-   Go to your repo on GitHub and confirm that your changes are visible in your Rmd **and** md files. If anything is missing, commit and push again.

## Packages

We'll use the **tidyverse** package for much of the data wrangling and visualisation and the data lives in the **dsbox** package.
These packages are already installed for you.
You can load them by running the following in your Console:

When you do this lab to install `dsbox` you can copy the code below into the console:

```
install.packages("devtools")
devtools::install_github("tidyverse/dsbox")
```

```{r message = FALSE}
library(tidyverse) 
library(dsbox) 
```

## Data

The datasets we'll use are called `dennys` and `laquinta` from the **dsbox** package.
Note that these data were scraped from [here](https://locations.dennys.com/) and [here](https://www.lq.com/en/findandbook/hotel-listings.html), respectively.

Since the datasets are distributed with the package, we don't need to load them separately; they become available to us when we load the package.
You can find out more about the datasets by inspecting their documentation, which you can access by running `?dennys` and `?laquinta` in the Console or using the Help menu in RStudio to search for `dennys` or `laquinta`.
You can also find this information [here](https://rstudio-education.github.io/dsbox/reference/dennys.html) and [here](https://rstudio-education.github.io/dsbox/reference/laquinta.html).

To help with our analysis we will also use a dataset on US states, which is located in your repository's `data` folder.

```{r}
states <- read_csv("data/states.csv")
```

Each observation in this dataset represents a state, including DC.
Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles).

# Exercises

1.  What are the dimensions of the Denny's dataset?
    (Hint: Use inline R code and functions like `nrow` and `ncol` to compose your answer.) What does each row in the dataset represent?
    What are the variables?
    - *The Denny's dataset is `r nrow(dennys)` rows by `r ncol(dennys)` columns. Each row is gives data on a single Denny's location. The variables are `adress`, `city`, `state`, `zip`, `longitude`, and `latitude`.*

2.  What are the dimensions of the La Quinta's dataset?
    What does each row in the dataset represent?
    What are the variables?
    - *The La Quinta dataset is `r nrow(laquinta)` rows by `r ncol(laquinta)` columns. Each row gives data on a single La Quinta Inn. The variables are `adress`, `city`, `state`, `zip`, `longitude`, and `latitude`.*

üß∂ ‚úÖ ‚¨ÜÔ∏è Knit, *commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.*

We would like to limit our analysis to Denny's and La Quinta locations in the United States.

3.  Take a look at the websites that the data come from (linked above).
    Are there any La Quinta's locations outside of the US?
    If so, which countries?
    What about Denny's?
- *There are La Quinta's locations in Canada, Mexico, China, New Zealand, Turkey, the UAE, Chile, Colombia, and Equador.*
  *Denny's, on the other hand, only has locations in the USA.*
4.  Now take a look at the data.
    What would be some ways of determining whether or not either establishment has any locations outside the US using just the data (and not the websites).
    Don't worry about whether you know how to implement this, just brainstorm some ideas.
    Write down at least one as your answer, but you're welcomed to write down a few options too.
- *I could possibly filter out only those rows that have a `state` value of one of the USA states, which would remove areas from other countries, unless they have the same state code as a U.S. state. I could also try only using those rows with only valid U.S. zipcodes if I knew how to determine which ones were.*
We will determine whether or not the establishment has a location outside the US using the `state` variable in the `dennys` and `laquinta` datasets.
We know exactly which states are in the US, and we have this information in the `states` dataframe we loaded.

5.  Find the Denny's locations that are outside the US, if any. To do so, filter the Denny's locations for observations where `state` is not in `states$abbreviation`. The code for this is given below. Note that the `%in%` operator matches the states listed in the `state` variable to those listed in `states$abbreviation`. The `!` operator means **not**. Are there any Denny's locations outside the US?

```{marginfigure}
"Filter for `state`s that are not in `states$abbreviation`."
```

```{r}
dennys %>%
  filter(!(state %in% states$abbreviation))
```
- *The resulting tibble has no rows, meaning that there are no Denny's locations outside the USA.*

6.  Add a country variable to the Denny's dataset and set all observations equal to `"United States"`. Remember, you can use the `mutate` function for adding a variable. Make sure to save the result of this as `dennys` again so that the stored data frame contains the new variable going forward.

```{marginfigure}
We don't need to tell R how many times to repeat the character string "United States" to fill in the data for all observations, R takes care of that automatically.
```

```{r}
dennys <- dennys %>%
  mutate(country = "United States")
```

7.  Find the La Quinta locations that are outside the US, and figure out which country they are in.
    This might require some googling.
    Take notes, you will need to use this information in the next exercise.

-   *From the website the La Quinta data was scraped from: *
    *Ecuador - Quito*
    *Colombia - Medellin*
    *Chile - Santiago*
    *UAE - Dubai*
    *Turkey - Bodrum, Giresun, Istanbul*
    *New Zealand - Aukland, Queenstown*
    *China - Taiyuan, Weifang*
    *Mexico - Aguascalientes, Ciudad Juarez, Poza Rica, Puebla, San Jose Chiapa, San Luis Potosi*
    *Canada - Richmond, Oshawa*

8.  Add a country variable to the La Quinta dataset.
    Use the `case_when` function to populate this variable.
    You'll need to refer to your notes from Exercise 7 about which country the non-US locations are in.
    Here is some starter code to get you going:

```{r eval = TRUE}
laquinta <- laquinta %>%
  mutate(country = case_when(
    state %in% state.abb     ~ "United States",
    state %in% c("ON", "BC") ~ "Canada",
    state == "ANT"           ~ "Colombia",
    state %in% c("AG", "CH", "VE", "PU", "SL", "QR", "NL") ~ "Mexico",
    state == "FM" ~ "Honduras"
  ))
```

- *So, for some reason the cities with La Quintas on the website the data was scraped from didn't match up with the dataframe itself; however, I have sorted the dataframe's values, using google searches and the filter command.*

üß∂ ‚úÖ ‚¨ÜÔ∏è Knit, *commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.*

Going forward we will work with the data from the United States only.
All Denny's locations are in the United States, so we don't need to worry about them.
However we do need to filter the La Quinta dataset for locations in United States.

```{r}
laquinta <- laquinta %>%
  filter(country == "United States")
```

9.  Which states have the most and fewest Denny's locations? What about La Quinta? Is this surprising? Why or why not?

```{r}
laquinta %>%
  group_by(state) %>%
  count() %>%
  arrange(n)
```
- *Of all the US states, Delaware has the fewest Denny's locations (1), and California has by far the most, at 403. I'm not too surprised that California, Texas, and Florida have the most, although I was partially expecting Texas to have the most.*
- *Meanwhile, in the La Quinta dataset, Texas does have the most locations of any US State, while Maine has the least. Both of these least values have been random states from the northeast, but I'm still surprised that Maine has so much fewer than Connecticut, which somehow has 6.*

Next, let's calculate which states have the most Denny's locations *per thousand square miles*.
This requires *joining* information from the frequency tables you created in Exercise 8 with information from the `states` data frame.

First, we count how many observations are in each state, which will give us a data frame with two variables: `state` and `n`.
Then, we join this data frame with the `states` data frame.
However note that the variables in the `states` data frame that has the two-letter abbreviations is called `abbreviation`.
So when we're joining the two data frames we specify that the `state` variable from the Denny's data should be matched `by` the `abbreviation` variable from the `states` data:

```{r}
dennys %>%
  count(state) %>%
  inner_join(states, by = c("state" = "abbreviation"))
```

Before you move on the the next question, run the code above and take a look at the output.
In the next exercise you will need to build on this pipe.

10. Which states have the most Denny's locations per thousand square miles? What about La Quinta?

```{r}
dennys %>%
  count(state) %>%
  inner_join(states, by = c("state" = "abbreviation")) %>%
  mutate(dennys_per_area = n/(1000*area)) %>%
  arrange(desc(dennys_per_area))

laquinta %>%
  count(state) %>%
  inner_join(states, by = c("state" = "abbreviation")) %>%
  mutate(dennys_per_area = n/(1000*area)) %>%
  arrange(desc(dennys_per_area))
```
- *DC has the most Denny's locations per thousand square kilometers, followed by Rhode Island, California, and Connecticut. In La Quinta, however, Rhode Island, Florida, Connecticut, and Maryland have the most per area.*

Next, we put the two datasets together into a single data frame.
However before we do so, we need to add an identifier variable.
We'll call this `establishment` and set the value to `"Denny's"` and `"La Quinta"` for the `dennys` and `laquinta` data frames, respectively.

```{r}
dennys <- dennys %>%
  mutate(establishment = "Denny's")
laquinta <- laquinta %>%
  mutate(establishment = "La Quinta")
```

Since the two data frames have the same columns, we can easily bind them with the `bind_rows` function:

```{r}
dn_lq <- bind_rows(dennys, laquinta)
```

We can plot the locations of the two establishments using a scatter plot, and color the points by the establishment type.
Note that the latitude is plotted on the x-axis and the longitude on the y-axis.

```{r}
ggplot(dn_lq, mapping = aes(x = longitude, y = latitude, color = establishment)) +
  geom_point()
```

The following two questions ask you to create visualizations.
These should follow best practices you learned in class, such as informative titles, axis labels, etc.
See <http://ggplot2.tidyverse.org/reference/labs.html> for help with the syntax.
You can also choose different themes to change the overall look of your plots, see <http://ggplot2.tidyverse.org/reference/ggtheme.html> for help with these.

11. Filter the data for observations in North Carolina only, and recreate the plot.
    You should also adjust the transparency of the points, by setting the `alpha` level, so that it's easier to see the overplotted ones.
    Visually, does Mitch Hedberg's joke appear to hold here?

```{r}
dn_lq %>%
  filter(state == "NC") %>%
  ggplot(aes(x = longitude, y = latitude, colour = establishment)) +
  geom_point(alpha = 0.5) +
  labs(title = "Locations of Denny's and La Quinta establishments", subtitle = "in North Carolina", x = "Latitude", y = "Longitude", colour = "Establishment")
```

- *Visually, Mitch Hedberg's joke doesn't hold up for most Denny's or La Quinta's locations, with only three seeming near-overlaps appearing on this graph.*

12. Now filter the data for observations in Texas only, and recreate the plot, with an appropriate `alpha` level.
    Visually, does Mitch Hedberg's joke appear to hold here?

```{r}
dn_lq %>%
  filter(state == "TX") %>%
  ggplot(aes(x = longitude, y = latitude, colour = establishment)) +
  geom_point(alpha = 0.1) +
  labs(title = "Locations of Denny's and La Quinta establishments", subtitle = "in Texas", x = "Latitude", y = "Longitude", colour = "Establishment")
```

- *This graph seems to support his joke more than the previous graph (although note the still-numerous unpaired La Quintas and Denny's locations); however, this graph is more zoomed-out than the previous one, and, due to the large number of establishments, we can see the major (and semi-major) cities of Texas, as well as the highways connecting them, making me think that the correlation might be an artifact of the size of the graph and the density of the establishments in cities. However, this is not conclusive from the graph, and is just my personal thoughts on the distribution.*

That's it for now!
In the next lab we will take a more quantitative approach to answering these questions.

üß∂ ‚úÖ ‚¨ÜÔ∏è Knit, *commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure you're happy with the final state of your work.*
